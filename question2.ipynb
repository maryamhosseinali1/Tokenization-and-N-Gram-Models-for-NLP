{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8cbe3166-95ed-4235-b0c9-7bb32c4568d9",
   "metadata": {},
   "source": [
    "### part3 - 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "496f6ad0-ba2d-4f98-af5e-cc92a7de6709",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import BertTokenizer\n",
    "\n",
    "\n",
    "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "\n",
    "\n",
    "with open('All_Around_the_Moon.txt', 'r') as file:\n",
    "    text = file.read()\n",
    "\n",
    "# tokenization\n",
    "tokens = tokenizer.tokenize(text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e0865cc9-9841-402b-994f-a367e241e428",
   "metadata": {},
   "source": [
    "### part3-2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "9aafe827-bf26-4f6c-8555-bd35dba20915",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BERT Vocabulary Size: 30522\n",
      "GPT-2 Vocabulary Size: 50257\n"
     ]
    }
   ],
   "source": [
    "from transformers import BertTokenizer, GPT2Tokenizer\n",
    "\n",
    "\n",
    "bert_tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "print(\"BERT Vocabulary Size:\", len(bert_tokenizer))\n",
    "\n",
    "\n",
    "gpt2_tokenizer = GPT2Tokenizer.from_pretrained('gpt2')\n",
    "print(\"GPT-2 Vocabulary Size:\", len(gpt2_tokenizer))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1effeff5-80cd-4bd2-9e97-df5aa7607924",
   "metadata": {},
   "source": [
    "### part3-3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "d0a1a990-9ef6-4104-b5cd-ca66b8a81741",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sentence1 ['this', 'darkness', 'is', 'absolutely', 'killing', '!', 'if', 'we', 'ever', 'take', 'this', 'trip', 'again', ',', 'it', 'must', 'be', 'about', 'the', 'time', 'of', 'the', 's', '##ne', '##w', 'moon', '!'] \n",
      "\n",
      "sentence2 ['this', 'is', 'a', 'token', '##ization', 'task', '.', 'token', '##ization', 'is', 'the', 'first', 'step', 'in', 'a', 'nl', '##p', 'pipeline', '.', 'we', 'will', 'be', 'comparing', 'the', 'token', '##s', 'generated', 'by', 'each', 'token', '##ization', 'model']\n"
     ]
    }
   ],
   "source": [
    "\n",
    "sentence1 = \"This darkness is absolutely killing! If we ever take this trip again, it must be about the time of the sNew Moon!\"\n",
    "tokenized_sentence1 = tokenizer.tokenize(sentence1)\n",
    "print('sentence1',tokenized_sentence1,'\\n')\n",
    "\n",
    "sentence2 = \"This is a tokenization task. Tokenization is the first step in a NLP pipeline. We will be comparing the tokens generated by each tokenization model\"\n",
    "tokenized_sentence2 = tokenizer.tokenize(sentence2)\n",
    "print('sentence2',tokenized_sentence2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "d1cd0cf4-d44a-4ec6-aaa1-990cf963321d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import GPT2Tokenizer\n",
    "\n",
    "\n",
    "tokenizer = GPT2Tokenizer.from_pretrained('gpt2')\n",
    "\n",
    "with open('All_Around_the_Moon.txt', 'r') as file:\n",
    "    text = file.read()\n",
    "\n",
    "# tokenization\n",
    "tokens = tokenizer.tokenize(text)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "372b37be-7aeb-49cb-b64c-277730e1d23c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sentence1 ['This', 'Ġdarkness', 'Ġis', 'Ġabsolutely', 'Ġkilling', '!', 'ĠIf', 'Ġwe', 'Ġever', 'Ġtake', 'Ġthis', 'Ġtrip', 'Ġagain', ',', 'Ġit', 'Ġmust', 'Ġbe', 'Ġabout', 'Ġthe', 'Ġtime', 'Ġof', 'Ġthe', 'Ġs', 'New', 'ĠMoon', '!'] \n",
      "\n",
      "sentence2 ['This', 'Ġis', 'Ġa', 'Ġtoken', 'ization', 'Ġtask', '.', 'ĠToken', 'ization', 'Ġis', 'Ġthe', 'Ġfirst', 'Ġstep', 'Ġin', 'Ġa', 'ĠN', 'LP', 'Ġpipeline', '.', 'ĠWe', 'Ġwill', 'Ġbe', 'Ġcomparing', 'Ġthe', 'Ġtokens', 'Ġgenerated', 'Ġby', 'Ġeach', 'Ġtoken', 'ization', 'Ġmodel']\n"
     ]
    }
   ],
   "source": [
    "\n",
    "sentence1 = \"This darkness is absolutely killing! If we ever take this trip again, it must be about the time of the sNew Moon!\"\n",
    "tokenized_sentence1 = tokenizer.tokenize(sentence1)\n",
    "print('sentence1',tokenized_sentence1,'\\n')\n",
    "\n",
    "sentence2 = \"This is a tokenization task. Tokenization is the first step in a NLP pipeline. We will be comparing the tokens generated by each tokenization model\"\n",
    "tokenized_sentence2 = tokenizer.tokenize(sentence2)\n",
    "print('sentence2',tokenized_sentence2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "536a24c7-182b-4b44-ad75-8159a2dcbd7e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
