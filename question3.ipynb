{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9f2d686c-99c1-4b2c-9fd0-fcef480c476e",
   "metadata": {},
   "source": [
    "### part1\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f456925f-2915-4172-b81d-c9e9498e5804",
   "metadata": {},
   "outputs": [],
   "source": [
    "import string\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.util import ngrams\n",
    "from nltk.corpus import stopwords\n",
    "from collections import Counter\n",
    "\n",
    "# reading text file\n",
    "with open('Tarzan.txt', 'r') as file:\n",
    "    text = file.read() \n",
    "    \n",
    "# remove punctuations function\n",
    "def remove_punc(text):\n",
    "    return ''.join([char for char in text if char not in string.punctuation])\n",
    "\n",
    "# removeing punctuation from the text\n",
    "clean_text = remove_punc(text)\n",
    "\n",
    "# Tokenization \n",
    "tokens = word_tokenize(clean_text)\n",
    "\n",
    "# removing stopwords\n",
    "stop_words = set(stopwords.words('english'))\n",
    "tokens = [token for token in tokens if token.lower() not in stop_words]\n",
    "\n",
    "# training n-gram model\n",
    "n = 1 \n",
    "ngrams_list = list(ngrams(tokens, n))  \n",
    "# generating n-grams\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d7d16972-75bf-4a5d-8c07-67442630deb2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(('Blake',), 411), (('upon',), 373), (('Tarzan',), 329), (('said',), 253), (('Ibn',), 238), (('would',), 228), (('Jad',), 227), (('Sir',), 205), (('one',), 203), (('man',), 199)]\n"
     ]
    }
   ],
   "source": [
    "ngrams_freq = Counter(ngrams_list) \n",
    "\n",
    "# print the most common n-grams (unigrams here)\n",
    "print(ngrams_freq.most_common(10))  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7503977-0743-4a84-baa3-e00e39eff2d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "246f97c3-be44-44f6-b159-1767a8a1a06e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import string\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.util import ngrams\n",
    "from nltk.corpus import stopwords\n",
    "from collections import Counter\n",
    "\n",
    "# remove punctuation function\n",
    "def remove_punc(text):\n",
    "    return ''.join([char for char in text if char not in string.punctuation])\n",
    "\n",
    "# reading text file\n",
    "with open('Tarzan.txt', 'r') as file:\n",
    "    text = file.read()\n",
    "\n",
    "# removing punctuation\n",
    "clean_text = remove_punc(text)\n",
    "\n",
    "# tokenization\n",
    "tokens = word_tokenize(clean_text)\n",
    "\n",
    "# removing stopwords\n",
    "stop_words = set(stopwords.words('english'))\n",
    "tokens = [token for token in tokens if token.lower() not in stop_words]\n",
    "\n",
    "# training bigram model\n",
    "n = 2 \n",
    "bigrams_list = list(ngrams(tokens, n))  # generating bigrams\n",
    "bigrams_freq = Counter(bigrams_list)  # count frequencies of bigrams\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "51039f8f-654c-45d8-bbe7-43c13eba6854",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Bigram: ('Ibn', 'Jad'), Frequency: 227, Probability: 0.006143270818110471\n",
      "Bigram: ('Tarzan', 'Apes'), Frequency: 57, Probability: 0.0015425834212876512\n",
      "Bigram: ('Project', 'Gutenberg™'), Frequency: 56, Probability: 0.001515520554247517\n",
      "Bigram: ('Sir', 'Richard'), Frequency: 52, Probability: 0.00140726908608698\n",
      "Bigram: ('Sir', 'James'), Frequency: 51, Probability: 0.001380206219046846\n",
      "Bigram: ('Sir', 'Malud'), Frequency: 39, Probability: 0.0010554518145652351\n",
      "Bigram: ('Project', 'Gutenberg'), Frequency: 28, Probability: 0.0007577602771237585\n",
      "Bigram: ('sir', 'knight'), Frequency: 27, Probability: 0.0007306974100836243\n",
      "Bigram: ('Princess', 'Guinalda'), Frequency: 27, Probability: 0.0007306974100836243\n",
      "Bigram: ('Knights', 'Nimmr'), Frequency: 25, Probability: 0.0006765716760033558\n"
     ]
    }
   ],
   "source": [
    "# calculating the total number of unique bigrams\n",
    "V = len(set(bigrams_list))\n",
    "\n",
    "# calculating the probability of each bigram\n",
    "bigram_prob = {}\n",
    "for bigram in bigrams_freq:\n",
    "    bigram_prob[bigram] = bigrams_freq[bigram] / len(bigrams_list)\n",
    "\n",
    "most_common_bigrams = bigrams_freq.most_common(10)\n",
    "for bigram, freq in most_common_bigrams:\n",
    "    \n",
    "    prob = bigram_prob[bigram]\n",
    "    print(f\"Bigram: {bigram}, Frequency: {freq}, Probability: {prob}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "f67bbf6f-0315-4b3c-a5f4-ec40a419888c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Bigram: ('of', 'the'), Frequency: 958, Smoothed Probability: 0.008392918092470879\n",
      "Bigram: ('in', 'the'), Frequency: 347, Smoothed Probability: 0.0030456053140561685\n",
      "Bigram: ('to', 'the'), Frequency: 306, Smoothed Probability: 0.002686783998319666\n",
      "Bigram: ('Ibn', 'Jad'), Frequency: 227, Smoothed Probability: 0.001995396585071283\n",
      "Bigram: ('and', 'the'), Frequency: 189, Smoothed Probability: 0.0016628304875594025\n",
      "Bigram: ('upon', 'the'), Frequency: 153, Smoothed Probability: 0.0013477678688639367\n",
      "Bigram: ('from', 'the'), Frequency: 149, Smoothed Probability: 0.0013127609112311071\n",
      "Bigram: ('he', 'had'), Frequency: 135, Smoothed Probability: 0.0011902365595162039\n",
      "Bigram: ('of', 'his'), Frequency: 133, Smoothed Probability: 0.001172733080699789\n",
      "Bigram: ('that', 'he'), Frequency: 121, Smoothed Probability: 0.0010677122078013004\n"
     ]
    }
   ],
   "source": [
    "\n",
    "n = 2 \n",
    "bigrams_list = list(ngrams(tokens, n))  \n",
    "bigrams_freq = Counter(bigrams_list)  \n",
    "\n",
    "# calculating the total number of unique bigrams\n",
    "V = len(set(bigrams_list))\n",
    "\n",
    "# calculating the probability of each bigram using Laplace smoothing\n",
    "k = 1  # Smoothing parameter\n",
    "bigram_prob_smoothed = {}\n",
    "for bigram in bigrams_freq:\n",
    "    # Laplace smoothing formula\n",
    "    bigram_prob_smoothed[bigram] = (bigrams_freq[bigram] + k) / (len(bigrams_list) + k*V)\n",
    "\n",
    "most_common_bigrams = bigrams_freq.most_common(10)\n",
    "for bigram, freq in most_common_bigrams:\n",
    "    prob_smoothed = bigram_prob_smoothed[bigram]\n",
    "    print(f\"Bigram: {bigram}, Frequency: {freq}, Smoothed Probability: {prob_smoothed}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f01e737-61a7-44e8-bac9-7807827808e7",
   "metadata": {},
   "source": [
    "### part3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "8ae08b4b-8fce-47ce-9353-8edb6e0ecee3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "First Sentence: Knowing well the windings of the trail he had been a great apes the Sepulcher and the Sepulcher\n",
      "Second Sentence: For half a day he lolled on the huge back and the Sepulcher and the Sepulcher and the Sepulcher and the\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "def remove_punc(text):\n",
    "    if isinstance(text, float):\n",
    "        return text\n",
    "    return ''.join([char for char in text if char not in string.punctuation])\n",
    "\n",
    "with open('Tarzan.txt', 'r') as file:\n",
    "    text = file.read()\n",
    "\n",
    "\n",
    "clean_text= remove_punc(text)\n",
    "\n",
    "tokens = word_tokenize(text_cleaned)\n",
    "\n",
    "n = 2  # \n",
    "bigrams_list = list(ngrams(tokens, n)) \n",
    "bigrams_freq = Counter(bigrams_list) \n",
    "\n",
    "\n",
    "V = len(set(tokens))\n",
    "\n",
    "\n",
    "k = 1 \n",
    "bigram_prob_smooth= {}\n",
    "for bigram in bigrams_freq:\n",
    "    bigram_prob_smooth[bigram] = (bigrams_freq[bigram] + k) / (len(bigrams_list) + k*V)\n",
    "\n",
    "most_common_bigrams = bigrams_freq.most_common(10)\n",
    "for bigram, freq in most_common_bigrams:\n",
    "    prob_smooth = bigram_prob_smooth[bigram]\n",
    "    \n",
    "\n",
    "\n",
    "# first Sentence\n",
    "sen1 = \"Knowing well the windings of the trail he\"\n",
    "for _ in range(10):  # complete to 10 tokens\n",
    "    \n",
    "    # last word in the current text\n",
    "    last_word = sen1.split()[-1]\n",
    "    \n",
    "    # getting all bigrams that start with last word\n",
    "    possible_bigrams = [bigram for bigram in bigram_prob_smoothed if bigram[0] == last_word]\n",
    "    if not possible_bigrams:\n",
    "        break     \n",
    "    # choosing next word base on the probabilities\n",
    "    next_bigram = max(possible_bigrams, key=lambda x: bigram_prob_smoothed[x])\n",
    "    sen1 += \" \" + next_bigram[1]\n",
    "\n",
    "# second sentence\n",
    "sen2 = \"For half a day he lolled on the huge back and\"\n",
    "for _ in range(10):  # complete to 10 tokens\n",
    "    \n",
    "    # last word in the current text\n",
    "    last_word = sen2.split()[-1]\n",
    "    # getting all bigrams that start with last word\n",
    "    possible_bigrams = [bigram for bigram in bigram_prob_smoothed if bigram[0] == last_word]\n",
    "    if not possible_bigrams:\n",
    "        break\n",
    "        \n",
    "    # choosing the next word base on probabilities\n",
    "    next_bigram = max(possible_bigrams, key=lambda x: bigram_prob_smoothed[x])\n",
    "    sen2 += \" \" + next_bigram[1]\n",
    "    \n",
    "print(\"First Sentence:\", sen1)\n",
    "print(\"Second Sentence:\", sen2)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48a610b2-0241-48ec-8d78-999c43e4f385",
   "metadata": {},
   "source": [
    "# part4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "151c0dce-cb13-40e1-879a-4af0e0f8ebc7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trigram: ('Project', 'Gutenberg™', 'electronic'), Frequency: 18, Probability: 0.0004871447902571042\n",
      "Trigram: ('said', 'Ibn', 'Jad'), Frequency: 16, Probability: 0.00043301759133964815\n",
      "Trigram: ('Project', 'Gutenberg', 'Literary'), Frequency: 13, Probability: 0.00035182679296346417\n",
      "Trigram: ('Gutenberg', 'Literary', 'Archive'), Frequency: 13, Probability: 0.00035182679296346417\n",
      "Trigram: ('Literary', 'Archive', 'Foundation'), Frequency: 13, Probability: 0.00035182679296346417\n",
      "Trigram: ('Gutenberg™', 'electronic', 'works'), Frequency: 12, Probability: 0.00032476319350473615\n",
      "Trigram: ('Project', 'Gutenberg™', 'License'), Frequency: 8, Probability: 0.00021650879566982408\n",
      "Trigram: ('Sheik', 'Ibn', 'Jad'), Frequency: 7, Probability: 0.00018944519621109606\n",
      "Trigram: ('menzil', 'Ibn', 'Jad'), Frequency: 7, Probability: 0.00018944519621109606\n",
      "Trigram: ('Zeyd', 'Ibn', 'Jad'), Frequency: 7, Probability: 0.00018944519621109606\n"
     ]
    }
   ],
   "source": [
    "import string\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.util import ngrams\n",
    "from nltk.corpus import stopwords\n",
    "from collections import Counter\n",
    "\n",
    "# remove punctuation function\n",
    "def remove_punc(text):\n",
    "    return ''.join([char for char in text if char not in string.punctuation])\n",
    "\n",
    "# reading text file\n",
    "with open('Tarzan.txt', 'r') as file:\n",
    "    text = file.read()\n",
    "\n",
    "# removing punctuation\n",
    "clean_text = remove_punc(text)\n",
    "\n",
    "# tokenization\n",
    "tokens = word_tokenize(clean_text)\n",
    "\n",
    "# removing stopwords\n",
    "stop_words = set(stopwords.words('english'))\n",
    "tokens = [token for token in tokens if token.lower() not in stop_words]\n",
    "\n",
    "# training trigram model\n",
    "n = 3\n",
    "trigrams_list = list(ngrams(tokens, n))  # generating trigrams\n",
    "trigrams_freq = Counter(trigrams_list)  # count frequencies of trigrams\n",
    "\n",
    "# calculating the total number of unique trigrams\n",
    "V = len(set(trigrams_list))\n",
    "\n",
    "# calculating the probability of trigram\n",
    "trigram_prob = {}\n",
    "for trigram in trigrams_freq:\n",
    "    trigram_prob[trigram] = trigrams_freq[trigram] / len(trigrams_list)\n",
    "\n",
    "most_common_trigrams = trigrams_freq.most_common(10)\n",
    "for trigram, freq in most_common_trigrams:\n",
    "    prob = trigram_prob[trigram]\n",
    "    print(f\"Trigram: {trigram}, Frequency: {freq}, Probability: {prob}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "5fb5fcd6-4bbb-40da-a008-21665d5c9f67",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trigram: ('Project', 'Gutenberg™', 'electronic'), Frequency: 18, Smoothed Probability: 0.00025982905982905983\n",
      "Trigram: ('said', 'Ibn', 'Jad'), Frequency: 16, Smoothed Probability: 0.00023247863247863248\n",
      "Trigram: ('Project', 'Gutenberg', 'Literary'), Frequency: 13, Smoothed Probability: 0.00019145299145299145\n",
      "Trigram: ('Gutenberg', 'Literary', 'Archive'), Frequency: 13, Smoothed Probability: 0.00019145299145299145\n",
      "Trigram: ('Literary', 'Archive', 'Foundation'), Frequency: 13, Smoothed Probability: 0.00019145299145299145\n",
      "Trigram: ('Gutenberg™', 'electronic', 'works'), Frequency: 12, Smoothed Probability: 0.00017777777777777779\n",
      "Trigram: ('Project', 'Gutenberg™', 'License'), Frequency: 8, Smoothed Probability: 0.00012307692307692307\n",
      "Trigram: ('Sheik', 'Ibn', 'Jad'), Frequency: 7, Smoothed Probability: 0.0001094017094017094\n",
      "Trigram: ('menzil', 'Ibn', 'Jad'), Frequency: 7, Smoothed Probability: 0.0001094017094017094\n",
      "Trigram: ('Zeyd', 'Ibn', 'Jad'), Frequency: 7, Smoothed Probability: 0.0001094017094017094\n"
     ]
    }
   ],
   "source": [
    "n = 3\n",
    "trigrams_list = list(ngrams(tokens, n))  \n",
    "trigrams_freq = Counter(trigrams_list)  \n",
    "\n",
    "# calculating the total number of unique trigrams\n",
    "V = len(set(trigrams_list))\n",
    "\n",
    "# calculating the probability of each trigram using Laplace smoothing\n",
    "k = 1  # Smoothing parameter\n",
    "trigram_prob_smoothed = {}\n",
    "for trigram in trigrams_freq:\n",
    "\t# Laplace smoothing formula\n",
    "\ttrigram_prob_smoothed[trigram] = (trigrams_freq[trigram] + k) / (len(trigrams_list) + k*V)\n",
    "\n",
    "most_common_trigrams = trigrams_freq.most_common(10)\n",
    "for trigram, freq in most_common_trigrams:\n",
    "\tprob_smoothed = trigram_prob_smoothed[trigram]\n",
    "\tprint(f\"Trigram: {trigram}, Frequency: {freq}, Smoothed Probability: {prob_smoothed}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "id": "28225ae2-a502-4574-8f70-02a2cdbd5c49",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "First Sentence: Knowing well the windings of the trail he took short cuts swinging through the enemies lines How passed\n",
      "Second Sentence: For half a day he lolled on the huge back and Usha tore through the neck and could not decipher their\n"
     ]
    }
   ],
   "source": [
    "import string  \n",
    "from nltk.tokenize import word_tokenize  \n",
    "from nltk.util import ngrams  \n",
    "from nltk.corpus import stopwords \n",
    "from collections import Counter  \n",
    "import random  \n",
    "\n",
    "\n",
    "def remove_punc(text):\n",
    "    return ''.join([char for char in text if char not in string.punctuation])  \n",
    "\n",
    "\n",
    "with open('Tarzan.txt', 'r') as file:\n",
    "    text = file.read() \n",
    "\n",
    "\n",
    "clean_text = remove_punc(text)  \n",
    "\n",
    "tokens = word_tokenize(clean_text)  \n",
    "\n",
    "\n",
    "n = 3  \n",
    "n_grams_list = list(ngrams(tokens, n))\n",
    "n_grams_freq = Counter(n_grams_list)  \n",
    "\n",
    "\n",
    "V = len(set(n_grams_list))\n",
    "\n",
    "\n",
    "k = 1  \n",
    "n_gram_prob_smooth = {}  \n",
    "for n_gram in n_grams_freq:  \n",
    "    n_gram_prob_smooth[n_gram] = (n_grams_freq[n_gram] + k) / (len(n_grams_list) + k*V)  # \n",
    "\n",
    "\n",
    "def generate_text(starting_text, n_gram_prob_smooth, n=3, max_words=10):\n",
    "    generated_text = starting_text  \n",
    "    # initializing generated text with starting text\n",
    "    \n",
    "    for _ in range(max_words):  \n",
    "        \n",
    "        last_words = tuple(generated_text.split()[-(n-1):])  \n",
    "        # getting the last 2 words\n",
    "        \n",
    "        possible_n_grams = [n_gram for n_gram in n_gram_prob_smooth if n_gram[:n-1] == last_words] \n",
    "        # find possible n-grams start with the last 2 words\n",
    "        \n",
    "        if not possible_n_grams:  \n",
    "            generated_text += \" \" + random.choice(list(n_gram_prob_smooth.keys()))[0]  \n",
    "            # choosing a random starting point\n",
    "            continue\n",
    "            \n",
    "        next_n_gram = random.choice(possible_n_grams)  \n",
    "        # choosing randomly from possible 3-grams\n",
    "        \n",
    "        generated_text += \" \" + next_n_gram[n-1]  \n",
    "    return generated_text  \n",
    "\n",
    "# Generating next 10 words for sentences\n",
    "sen1 = \"Knowing well the windings of the trail he\"\n",
    "generated_text1 = generate_text(sen1, n_gram_prob_smooth, n=3)\n",
    "\n",
    "\n",
    "sen2 = \"For half a day he lolled on the huge back and\"\n",
    "generated_text2 = generate_text(sen2, n_gram_prob_smooth, n=3)\n",
    "\n",
    "print(\"First Sentence:\", generated_text1)  \n",
    "print(\"Second Sentence:\", generated_text2)  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "ca1e7905-a54b-4372-8d02-80ff784e052c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5-gram: ('Project', 'Gutenberg', 'Literary', 'Archive', 'Foundation'), Frequency: 13, Probability: 0.000351845837393093\n",
      "5-gram: ('phrase', '“', 'Project', 'Gutenberg', '”'), Frequency: 4, Probability: 0.00010826025765941322\n",
      "5-gram: ('United', 'States', 'check', 'laws', 'country'), Frequency: 3, Probability: 8.119519324455992e-05\n",
      "5-gram: ('water', 'hole', 'smooth', 'round', 'rocks'), Frequency: 3, Probability: 8.119519324455992e-05\n",
      "5-gram: ('use', 'anyone', 'anywhere', 'United', 'States'), Frequency: 2, Probability: 5.413012882970661e-05\n",
      "5-gram: ('anyone', 'anywhere', 'United', 'States', 'parts'), Frequency: 2, Probability: 5.413012882970661e-05\n",
      "5-gram: ('anywhere', 'United', 'States', 'parts', 'world'), Frequency: 2, Probability: 5.413012882970661e-05\n",
      "5-gram: ('United', 'States', 'parts', 'world', 'cost'), Frequency: 2, Probability: 5.413012882970661e-05\n",
      "5-gram: ('States', 'parts', 'world', 'cost', 'almost'), Frequency: 2, Probability: 5.413012882970661e-05\n",
      "5-gram: ('parts', 'world', 'cost', 'almost', 'restrictions'), Frequency: 2, Probability: 5.413012882970661e-05\n"
     ]
    }
   ],
   "source": [
    "import string\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.util import ngrams\n",
    "from nltk.corpus import stopwords\n",
    "from collections import Counter\n",
    "\n",
    "# remove punctuation function\n",
    "def remove_punc(text):\n",
    "    return ''.join([char for char in text if char not in string.punctuation])\n",
    "\n",
    "# reading text file\n",
    "with open('Tarzan.txt', 'r') as file:\n",
    "    text = file.read()\n",
    "\n",
    "# removing punctuation\n",
    "clean_text = remove_punc(text)\n",
    "\n",
    "# tokenization\n",
    "tokens = word_tokenize(clean_text)\n",
    "\n",
    "# removing stopwords\n",
    "stop_words = set(stopwords.words('english'))\n",
    "tokens = [token for token in tokens if token.lower() not in stop_words]\n",
    "\n",
    "# training 5-gram model\n",
    "n = 5\n",
    "five_grams_list = list(ngrams(tokens, n))  # generating 5-grams\n",
    "five_grams_freq = Counter(five_grams_list)  # count frequencies of 5-grams\n",
    "\n",
    "# calculating the total number of unique 5-grams\n",
    "V = len(set(five_grams_list))\n",
    "\n",
    "# calculating the probability of each 5-gram\n",
    "five_gram_prob = {}\n",
    "for five_gram in _5_grams_freq:\n",
    "    five_gram_prob[five_gram] = five_grams_freq[five_gram] / len(five_grams_list)\n",
    "\n",
    "most_common_5_grams = five_grams_freq.most_common(10)\n",
    "for five_gram, freq in most_common_5_grams:\n",
    "    prob = five_gram_prob[five_gram]\n",
    "    print(f\"5-gram: {five_gram}, Frequency: {freq}, Probability: {prob}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "id": "79ed62f1-eade-4b38-9f76-e185bfe59492",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5-gram: ('Project', 'Gutenberg', 'Literary', 'Archive', 'Foundation'), Frequency: 13, Smoothed Probability: 0.0001897018970189702\n",
      "5-gram: ('phrase', '“', 'Project', 'Gutenberg', '”'), Frequency: 4, Smoothed Probability: 6.775067750677507e-05\n",
      "5-gram: ('United', 'States', 'check', 'laws', 'country'), Frequency: 3, Smoothed Probability: 5.4200542005420054e-05\n",
      "5-gram: ('water', 'hole', 'smooth', 'round', 'rocks'), Frequency: 3, Smoothed Probability: 5.4200542005420054e-05\n",
      "5-gram: ('use', 'anyone', 'anywhere', 'United', 'States'), Frequency: 2, Smoothed Probability: 4.065040650406504e-05\n",
      "5-gram: ('anyone', 'anywhere', 'United', 'States', 'parts'), Frequency: 2, Smoothed Probability: 4.065040650406504e-05\n",
      "5-gram: ('anywhere', 'United', 'States', 'parts', 'world'), Frequency: 2, Smoothed Probability: 4.065040650406504e-05\n",
      "5-gram: ('United', 'States', 'parts', 'world', 'cost'), Frequency: 2, Smoothed Probability: 4.065040650406504e-05\n",
      "5-gram: ('States', 'parts', 'world', 'cost', 'almost'), Frequency: 2, Smoothed Probability: 4.065040650406504e-05\n",
      "5-gram: ('parts', 'world', 'cost', 'almost', 'restrictions'), Frequency: 2, Smoothed Probability: 4.065040650406504e-05\n"
     ]
    }
   ],
   "source": [
    "n = 5\n",
    "five_grams_list = list(ngrams(tokens, n))  \n",
    "five_grams_freq = Counter(five_grams_list)  \n",
    "\n",
    "# calculating the total number of unique 5-grams\n",
    "V = len(set(five_grams_list))\n",
    "\n",
    "# calculating the probability of each 5-gram using Laplace smoothing\n",
    "k = 1  # Smoothing parameter\n",
    "five_gram_prob_smoothed = {}\n",
    "for five_gram in _5_grams_freq:\n",
    "\t# Laplace smoothing formula\n",
    "\tfive_gram_prob_smoothed[five_gram] = (five_grams_freq[five_gram] + k) / (len(five_grams_list) + k*V)\n",
    "\n",
    "most_common_5_grams = five_grams_freq.most_common(10)\n",
    "for five_gram, freq in most_common_5_grams:\n",
    "\tprob_smoothed = five_gram_prob_smoothed[five_gram]\n",
    "\tprint(f\"5-gram: {five_gram}, Frequency: {freq}, Smoothed Probability: {prob_smoothed}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "id": "60ee98f8-70fe-4e38-9e39-b63bf2eed268",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "First Sentence: Knowing well the windings of the trail he took short cuts swinging through the branches of the trees\n",
      "Second Sentence: For half a day he lolled on the huge back and a suddenly down felled heard when I far up Apes\n"
     ]
    }
   ],
   "source": [
    "import string \n",
    "from nltk.tokenize import word_tokenize \n",
    "from nltk.util import ngrams  \n",
    "from nltk.corpus import stopwords  \n",
    "from collections import Counter  \n",
    "import random \n",
    "\n",
    "def remove_punc(text):\n",
    "  \n",
    "    return ''.join([char for char in text if char not in string.punctuation])  \n",
    "\n",
    "with open('Tarzan.txt', 'r') as file:\n",
    "    text = file.read()  \n",
    "\n",
    "clean_text = remove_punc(text)  \n",
    "tokens = word_tokenize(clean_text)  \n",
    "\n",
    "\n",
    "n = 5  \n",
    "n_grams_list = list(ngrams(tokens, n))\n",
    "n_grams_freq = Counter(n_grams_list)  \n",
    "\n",
    "V = len(set(n_grams_list))  \n",
    "\n",
    "k = 1  \n",
    "n_gram_prob_smooth = {}  \n",
    "for n_gram in n_grams_freq:  \n",
    "    n_gram_prob_smooth[n_gram] = (n_grams_freq[n_gram] + k) / (len(n_grams_list) + k*V)  \n",
    "\n",
    "def generate_text(starting_text, n_gram_prob_smooth, n=5, max_words=10):\n",
    "    generated_text = starting_text \n",
    "    # initializing the generated text with the starting text\n",
    "    \n",
    "    for _ in range(max_words): \n",
    "        last_words = tuple(generated_text.split()[-(n-1):])\n",
    "        # get the last n-1 words\n",
    "        \n",
    "        possible_n_grams = [n_gram for n_gram in n_gram_prob_smooth if n_gram[:n-1] == last_words]\n",
    "        # find possible 5-grams start with last 4 words\n",
    "        \n",
    "        if not possible_n_grams:  \n",
    "            generated_text += \" \" + random.choice(list(n_gram_prob_smooth.keys()))[0]  \n",
    "            # choosing a random starting point\n",
    "            \n",
    "            continue\n",
    "        next_n_gram = random.choice(possible_n_grams) \n",
    "        # choosing random from possible n-grams\n",
    "        \n",
    "        generated_text += \" \" + next_n_gram[n-1] \n",
    "    return generated_text  \n",
    "\n",
    "# generating next 10 words for sentences\n",
    "sen1 = \"Knowing well the windings of the trail he\"\n",
    "generated_text1 = generate_text(sen1, n_gram_prob_smooth)\n",
    "\n",
    "\n",
    "sen2 = \"For half a day he lolled on the huge back and\"\n",
    "generated_text2 = generate_text(sen2, n_gram_prob_smooth)\n",
    "\n",
    "print(\"First Sentence:\", generated_text1)  \n",
    "print(\"Second Sentence:\", generated_text2) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f60e9c6-b55a-417b-9d0b-2cc58fb5771e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
