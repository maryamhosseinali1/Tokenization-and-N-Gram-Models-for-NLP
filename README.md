# **Tokenization and N-Gram Models for NLP**

## **Project Overview**
This project covers key NLP concepts, focusing on tokenization strategies and n-gram language modeling. The aim is to understand the impact of different tokenization approaches, develop n-gram models for text generation, and conduct sentiment analysis using n-grams.

## **Table of Contents**
1. [Objectives](#objectives)
2. [Tokenization with Regular Expressions](#tokenization-with-regular-expressions)
3. [Tokenization in BERT and GPT Models](#tokenization-in-bert-and-gpt-models)
4. [N-Gram Language Modeling](#n-gram-language-modeling)
5. [Sentiment Analysis using N-Grams](#sentiment-analysis-using-n-grams)
6. [Results and Interpretation](#results-and-interpretation)
7. [Tools and Dependencies](#tools-and-dependencies)

## **Objectives**
The project's goals are to:
- Develop a regex-based tokenizer to explore tokenization challenges.
- Analyze tokenization in BERT and GPT and compare their differences.
- Build n-gram models to generate text sequences and study n-gram probabilities.
- Use n-gram models for sentiment analysis on a labeled dataset of user reviews.

---

## **Tokenization with Regular Expressions**
### **Task**:
Implement a word-based custom tokenizer using regular expressions and explore challenges like abbreviations, punctuation, and handling special characters.

### **Key Steps**:
1. Created a regex-based tokenizer to split text into words, considering issues like punctuation, abbreviations (e.g., "M.Sc."), and dates.
2. Enhanced the tokenizer to better handle special cases such as hashtags, contractions, and composite words.

### **Outcome**:
Improved tokenization accuracy, but limitations like ambiguity in phrases and special cases persisted.

---

## **Tokenization in BERT and GPT Models**
### **Task**:
Compare and analyze how BERT and GPT tokenize text using their respective subword tokenization algorithms (WordPiece for BERT, BPE for GPT).

### **Key Steps**:
1. Studied subword tokenization strategies for BERT and GPT, focusing on vocabulary size and handling of unknown words.
2. Implemented and compared tokenization outputs from both models on sample sentences, highlighting differences in vocabulary and tokenization.

### **Outcome**:
BERT and GPT use different vocabularies and tokenization algorithms, resulting in varying tokenization outputs. BERT uses a smaller vocabulary (30,522 tokens), while GPT has a larger one (50,257 tokens).

---

## **N-Gram Language Modeling**
### **Task**:
Develop n-gram language models using text data and generate sequences based on bigram, trigram, and 5-gram probabilities.

### **Data**:
- Text corpus from *Tarzan, Lord of the Jungle*.

### **Key Steps**:
1. **Preprocessing**: Tokenized the text and removed punctuation and stopwords.
2. **Building n-Gram Models**: Built bigram, trigram, and 5-gram models and calculated their probabilities using Laplace smoothing to mitigate data sparsity.
3. **Text Generation**: Generated text sequences using n-gram models and compared coherence across different `n` values.

### **Outcome**:
Text generated by bigrams was less coherent, while trigrams and 5-grams produced more contextually accurate sequences.

---

## **Sentiment Analysis using N-Grams**
### **Task**:
Use n-gram features to predict sentiment from a labeled dataset of user reviews from the Google Play Store.

### **Key Steps**:
1. **Data Preparation**: Divided reviews into training (80%) and testing (20%) sets.
2. **Model Training**: Built separate n-gram models for positive and negative sentiments.
3. **Sentiment Prediction**: Calculated n-gram probabilities for each review to predict the sentiment label.

### **Outcome**:
The model achieved an accuracy of **76%** on the test set, indicating reasonable performance in sentiment prediction.

---

## **Results and Interpretation**
- **Tokenization**: Regex-based tokenization faced challenges with special cases; BERT and GPT tokenizers provided a deeper understanding of subword units.
- **N-Gram Modeling**: Increasing `n` led to more coherent text generation, but also introduced data sparsity issues and required more computational resources.
- **Sentiment Analysis**: N-gram-based sentiment models achieved moderate accuracy, showing promise for further refinement.

---

## **Tools and Dependencies**
- **Python Libraries**: `re` (for regex-based tokenization), `transformers` (for BERT and GPT tokenization), `nltk` (for n-gram modeling).
- **Data Manipulation**: `pandas`.
- **Text Processing**: `nltk` for tokenization, stopword removal, and smoothing in n-gram models.

---


